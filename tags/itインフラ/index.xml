<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ITインフラ on @uyorumの雑記帳</title>
    <link>https://blog.uyorum.net/tags/it%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9/</link>
    <description>Recent content in ITインフラ on @uyorumの雑記帳</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <copyright>uyorum All Right Reserved.</copyright>
    <lastBuildDate>Wed, 16 Jan 2019 13:06:37 +0900</lastBuildDate><atom:link href="https://blog.uyorum.net/tags/it%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ファシリティまわりの設計時に考慮すること(続き)</title>
      <link>https://blog.uyorum.net/post/designing-facility-layer-02/</link>
      <pubDate>Wed, 16 Jan 2019 13:06:37 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/designing-facility-layer-02/</guid>
      <description>前回は主にラックと電源についてまとめた．その続き．
ネットワーク イーサネットを前提としてまとめる．
規格 必要な帯域を確保できるようイーサネットの規格を選択する．イーサネットの規格によってケーブルの種類が決まってくる．ケーブルの種類は以下のような要素に影響してくるので，それらも考慮して決める．
 ケーブルの最大長 ケーブルの太さ ケーブルの固さ ケーブルの価格  ケーブルの太さや固さケーブリングのしやすさに大きく影響してくるので特に気をつけたい． また機器側がSFPポートの場合は，ケーブル規格に適合するSFPトランシーバーを選択しなければならない．
長さ エアフローの確保やメンテナンス性を考え，ケーブルはポート間を最短距離で結ぶのではなく，ラックの両脇を通すことになる．それを考慮してケーブルの長さを決める．そのため同一ラック内の場合でもケーブルの長さは最低でも3〜5mくらいにしておいた方が無難． ラック間の接続の場合はさらに注意が必要．データセンターによってはラック間のケーブルを通す場所を通す場所が決まっていることがある．隣接ラックの場合はラック側面のパネルを全部または一部外すことで，ほぼ最短距離でケーブルを設置することができるが，データセンターによってはそれを許可していないことがあるため，事前に確認しておいた方がよい．それが禁止されている場合またはラックが隣接していない場合はケーブルを天井または床下に通すことになる．その場合も必要な長さをデータセンターに確認した方がよい．(データセンターに依頼しなければラック間のケーブルを敷設をしてはいけないこともある)
保守サポート サポート内容 提供されている保守サポートは製品によって異なる．必要な保守内容をよく検討した上で保守を契約する．製品によっては保守の内容がハードウェア/ソフトウェアの2つに分かれている場合があるので確認する．
 ハードウェア保守  部品の故障などに対応する   ソフトウェア保守  OSやファームウェアの不具合などに対応する    受付時間 サポート窓口の受付時間もいくつかの種類がある．よくある種別は以下の2通り．
 24時間365日 平日日中  クリティカルな機器のサポート受付時間が平日日中のみとならないようよく確認する．
センドバック/オンサイト 部品の故障が発生した場合は部品交換または機器ごと交換となるが，その方法は大きく分けて2通りある．
 センドバック保守
機器または部品を返送し，修理済の機器や交換用の機器を送り返してもらう．修理までに時間がかかるうえ，機器を送り返してもらうまでその機器は使用できない． オンサイト保守
機器が設置されている場所に保守技術者が来てくれ，その場で機器または部品の交換を行う．センドバック保守より修理までの時間が早いが，保守料金はこちらの方が高い．  その他 エアフロー ケーブリングをしやすくするため，ポート面を揃えた方がよい．かつ冷却性を高めるため排気面も揃えるべき． 機器によっては注文時に吸気面，排気面を選択できるが，選択できない機器もあるため設置する機器でポート面と排気面を揃えるように選定する．
付属品 機器によっては以下のようなパーツは別売りとなっている場合がある．データシートなどで付属品を確認し，漏れがないようにする．
 ラックマウントキット(羽，レールなど) 電源ケーブル SFPトランシーバー (サーバなど)フロントパネル  以上
  インフラ/ネットワークエンジニアのためのネットワーク・デザインパターン 実務で使えるネットワーク構成の最適解27  amazon.co.jp     </description>
    </item>
    
    <item>
      <title>ファシリティまわりの設計時に考慮すること</title>
      <link>https://blog.uyorum.net/post/designing-facility-layer-01/</link>
      <pubDate>Sun, 16 Dec 2018 12:58:08 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/designing-facility-layer-01/</guid>
      <description>最近とあるシステムを新規構築するにあたりファシリティまわりの設計をすることになった． 個人的に初めてのレイヤで，かつ暗黙知が多すぎて苦労したので備忘録的にまとめておく．
筆者の知識  ラッキング，ケーブリング作業は行ったことがある 機器選定やラック設計は行ったことがない  扱う領域 このエントリでは自分が考慮した以下の領域についてまとめる．ただし現場ごとの習慣や組織構造などによって考慮すべきことが異なるはずなので注意．
 ラック設計 機器選定 ケーブル規格  前提  機器はデータセンターに設置する データセンターは自社所有だが異なる部署が管理している(基本的に設備は勝手にいじれない) サーバ室はコロケーション  ラック ラック寸法 機器を設置する部分の幅と高さはEIA規格という規格に準拠して設計されていることが多い(JIS規格というものもあるようだが自分は見たことがない)． EIA規格では幅19インチ，高さが1.75インチの倍数と定めており，だいたいの機器の寸法もこれに従って設計されている．この1.75インチを1単位として1U(ユニット)と呼ぶ． 機器を設置しようとしているラックがEIA規格に準拠していることを確認すればよい．またはEIA準拠のラックを調達する．
ここで注意なのが，EIA規格は機器を設置する部分の幅と高さを定めているが，奥行については定めていないということ．設置しようとしている機器の奥行がラックに収まるサイズかは別途チェックする必要がある． また，ラックの扉の構造や(機器を取り付ける部分ではなく)ラック自体の横幅もラックによって異なる． 通常はネットワークケーブルや電源コードが機器の前後にせり出すうえ，左右にケーブルを這わせたりするため十分なスペースがあるか実物を見て確認しておいた方がよいかもしれない．
耐荷重 通常はラック(というよりはデータセンターか)に対して設置できる機器の総重量が定められている．(自分のまわりではデータセンターにより500kg~1000kgほどの幅がある) 機器のメーカサイトで公開されているデータシートに重量が記載されているため耐荷重を超えていないか確認すべき．
電源 データセンターは電力会社含め，電源設備の故障に備えて複数系統の電源をラック内に設置できることが多い．電源を設置する際には以下の点を考慮する．
電圧 機器によって対応する電圧が異なる．100Vか200Vまたはその両方に対応する機器が存在する．これも機器のデータシートに記載されているため確認しておく． 当然使用する電圧ごとに電源系統をラック内に電源をひきこんでおく必要がある．(ラック内で変圧するような機器もあるかもしれないが自分は知らない)
機器が100Vと200Vの両方に対応する場合にどちらを使えばよいか，自分の場合は主に以下の2点を考慮して決めた
伝送効率 同じ消費電力(W)の場合，100Vと200Vでは200Vの方が電流が小さい分，伝送路の抵抗による電圧降下が小さくなる．(=伝送効率がよくなる)1 サーバのような消費電力の大きい機器の場合はできるだけ200Vで動かすようにした．
電源敷設の手間 同一ラック内で100Vと200Vの機器が混在する場合，そのラックへは100Vと200Vの電源を両方ひきこまなければならない．使用する電圧はできるだけ統一した方が電源設備がシンプルになるうえ，コストダウンにつながる． ただし電源を200Vしか用意しない場合，ノートPCなどで作業を行う場合の電源は別途考えなければならない．
電源口の設置場所 電源口はコンセントバーやPDUと呼ばれる機器をラック内に設置することで確保する． 設置方法はラック側面に設置する方法とラックにマウントする方法がある．
ラック側面に設置する場合，接続口が上下に広く配置されるので電源コードが届かないということはまずない．しかし，コンセントバー自身やそれに挿さる電源コードが機器の出し入れ等の作業の邪魔になることがある．
ラックにマウントする場合，ユニットを消費するうえ，設置位置と電源コードの長さによっては電源コードが届かないということが発生しうる．しかしそれに挿さる電源コード自身が邪魔になるということは少ない．
電源容量 はラックごとに使用できる電源の容量が設定されている．もしこれをオーバーして電力を消費した場合，ブレーカーが落ちてしまう． ラック内の機器の配置を考えるときは機器の消費電力の合計がこの容量を超えないようにする必要がある．
消費電力，有効電力，皮相電力，力率  消費電力(有効電力)
ある機器でどれくらいの電力を消費するか．単位はW．だいたいはデータシートに記載されている．ワークロードの量によって消費する電力は変動するため，実際に機器がどれくらいの電力を消費するか事前に見積もるのは難しい．(データシートには標準電力，最大電力のように分けて記載されているものもあり，参考にはなる)
自分の場合は最大電力の50%くらいで見積もった．消費電力を測る機能を持つPDUもあるため，設置後も消費電力をモニタリングし続けるのがいいだろう． 皮相電力
電源設備が供給することのできる電力．単位はVA．例えば6000VAの場合は200Vで合計60Aの電流を流すことができるということになる． 力率
交流回路では必ずしも6000VAの皮相電力で6000Wの有効電力が得られるわけではない．機器ごとに力率という値が存在し，「皮相電力x力率」の値が実際に機器で使用できる電力となる．
つまり皮相電力6000VAのラックに消費電力6000Wの機器は設置できない(1台でこれだけ電力を消費する機器はなかなかないと思うが)ということ．設置する機器の力率を考慮する必要がある．(しかし多くの場合，力率はデータシートにも記載されていない)
皮相電力6000VAのラックには，例えば力率60%/消費電力3600Wの機器まで設置できることになる．  電源端子の規格 家庭用機器にある平型2P以外にも，データセンターにおいては様々な端子を扱う．端子形状はデータシートに記載されているためプラグとソケットに規格を合わせるように気をつける．
電源端子の形状については以下のページがよくまとまっている．
特注電源ケーブル　|　エイム電子株式会社
長くなったので今回はここまで</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (6-9章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap6-9/</link>
      <pubDate>Thu, 29 Jun 2017 21:52:49 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap6-9/</guid>
      <description>オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．太字は自分の感想， 斜字体は本からの引用 ，そのほかは本の要約など．
6~9章は各領域での設計パターンやプラクティスを整理している
サーバーのプロビジョニング  サーバーに含まれるもの  ソフトウェア 構成/設定 データ  ログもここに含まれる     インフラストラクチャはサーバのアップデート，交換，削除のプロセスを通して，一貫して「データ」へのアクセス性を提供しなければならない プロセスのさまざまな部分にかかる時間を計測するようにしなければならない  これはいろんなところに言えそう．次からは時間を計測することを考えるようにしようと思う    サーバーのテンプレート管理  構築方法  原始イメージでサーバーを作成し設定を変更する ほかのサーバーに原始イメージディスクをマウントし変更を加える  chrootを使う ブート，シャットダウンの時間を省略できる テンプレート上にログが作成されないのでわざわざ削除する必要がない Netflix/aminator: A tool for creating EBS AMIs. This tool currently works for CentOS/RedHat Linux images and is intended to run on an EC2 instance. このやり方は思い付かなかった     テンプレート自体にもバージョン番号を付け，各サーバーがどのテンプレートから作成されたか追跡できるようにする テンプレートをアップデートしたら既存のサーバーも作成しなおせ，さもないと構成ドリフトが発生する  これをやるのはすごく難しいと思うのだが．サービスを停止せずにサーバーを入れ替えなければならない テンプレートに変更を加えたら既存サーバーにも同じ変更を加えるようにするのが妥協ラインかな…    サーバーのアップデート/変更  プッシュ同期 プル同期  変更後のテストもサーバーから自発的に実行できる必要がある？  サーバー上で動かしても問題なさそう：aelsabbahy/goss: Quick and Easy server testing/validation あるいはモニタリングにまかせる   異常が起こった場合の切り戻しはどうやってトリガーする？   マスターレス構成管理  SPoFがなくなる   サーバー作成直後の設定とサーバーのアップデートは必ずしも同一の仕組みとは限らない，という前提でこの本は書かれている気がする  インフラストラクチャ定義  適切なスタックにインフラを分割し，定義，実装する 人々が変更を加えるのを恐れるようになったら，インフラストラクチャ定義がモノリシックになってきていると考えることができる スタックの共有(DBサーバの共有など)は避けるべき  5章で説明された通りどうしても共有されるサービスは存在する．その場合はサービスレベルを定める   アプリケーションコードとインフラストラクチャコードを統一的に管理する  Googleのような巨大リポジトリでの開発には，呼び出し先の変更と同時に呼び出し元も変更できるという利点がある．1 これと似たような考え方か．   既存の設計パターンを当てはめようとすると，かえって複雑になる場合がある．設計を見直すことも必要  管理しやすようにやり方を変える．手段と目的が逆転してるように感じるが大事    参考文献  Kief Morris, Infrastructure as Code クラウドにおけるサーバ管理の原則とプラクティス, 長尾高弘訳, オライリー・ジャパン, 2017    Infrastructure as Code ―クラウドにおけるサーバ管理の原則とプラクティス  amazon.</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (5章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap5/</link>
      <pubDate>Wed, 21 Jun 2017 22:47:39 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap5/</guid>
      <description>オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．太字は自分の感想， 斜字体は本からの引用 ，そのほかは本の要約など．
インフラストラクチャサービス，ツールが満たすべき条件  外部定義を使えるツールを選ぶ  DBに設定を保持するツールはどうすればいいか  設定をyamlなどで構造化してテキストに保存．それを読み込んでAPIを発行，サービスへ設定を反映させる．確実に両者が同一の内容であることを確信するいは逆(サービスからyamlへ)もできる必要がある インポート/エクスポートを使う．(エクスポートした設定情報は大抵は人間が読むようにはできていない．独自のフォーマットで書かれていて容易にパースできない) (本に書いてある例)SeleniumのようなものでGUIを操作する   いずれにしても辛そう．よほどのことがない限りそのようなツールは選択すべきでない   インフラストラクチャがダイナミックだという前提で作られたツールを選ぶ  サービス自身とサービスが管理するものの変化に柔軟に自動的に追従，対処できるもの   ライセンスがクラウド互換になっている製品を選ぶ  ここでは主に柔軟性に関するもの   疎結合をサポートする製品を選ぶ  チーム間でのサービスの共有  チーム間で共有される可能性のあるサービス  モニタリング，CI，バグ追跡(BTS？)，DNS，アーティファクトリポジトリ(構成レジストリ？)   それを使うチームの要件，使われ方の特性などが様々になり，それ自体が小さなパブリックサービスのようにみなせるかもしれない  それぞれサービスレベルやサービス仕様を定めてチームに使ってもらう ふつうにマイクロサービスアーキテクチャの考え方みたい    モニタリング  モニタリングの目標は，必要とする人に必要なときに適切な情報を提供すること  つまり，モニタリングシステムの設計を始める前に想定ユーザを決めるフェーズがあるということ．今までは考えたことなかった   個々のイベントは問題ないが，それが頻発する場合は問題がある可能性がある場合，頻度に関する閾値を設ける (a)日常の仕事を続行する，(b)大声を出して今していることを中断し，対策に乗り出す どちらを取るべきかをすぐに判断できるようなのでなければならない 複数のサーバーに関連性があることを自動的にタギングしなければならない  Zabbixでもネットワークディスカバリ，ローレベルディスカバリ，AgentのUserParameterあたりを使えば自動でタギングできるが，ダッシュボードを動的に生成できないのが辛い  つまりZabbixはInfrastructure as Codeに適していない   DataDogやMackerel(おそらくNewRelicも)はこのへんの考え方が前提になってる Prometheusはこのあたりどうなんだろう．Grafanaと組み合わせれば普通にできそう(それを言えばZabbixでもいいのだが)    サービスディスカバリ  サーバーサイドサービスディスカバリ  ロードバランサが結局ボトルネックになったりする   クライアントサイドサービスディスカバリ  通常，こちらの方がアプリケーションは複雑になる アプリケーションのlocalhostにロードバランサを用意すればいろいろ解決しそう    参考文献  Kief Morris, Infrastructure as Code クラウドにおけるサーバ管理の原則とプラクティス, 長尾高弘訳, オライリー・ジャパン, 2017    Infrastructure as Code ―クラウドにおけるサーバ管理の原則とプラクティス  amazon.</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (4章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap4/</link>
      <pubDate>Sat, 17 Jun 2017 18:17:36 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap4/</guid>
      <description>&lt;p&gt;オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．&lt;strong&gt;太字は自分の感想&lt;/strong&gt;， &lt;em&gt;斜字体は本からの引用&lt;/em&gt; ，そのほかは本の要約など．&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (3章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap3/</link>
      <pubDate>Thu, 15 Jun 2017 23:14:55 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap3/</guid>
      <description>オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．太字は自分の感想， 斜字体は本からの引用 ，そのほかは本の要約など．
ツールの要件  他のツールを連携しやすいこと  コマンドライン引数や環境変数などでの入力，パースしやすい結果出力 設定の外在化   自動実行しやすいこと  冪等性など 失敗したらわかる   周辺のツールとの連携しやすさは意識して考慮に入れてなかった 考え方はすごくよくわかる．UNIX哲学に通じている  構成レジストリ  コンフィグ定義ツールが提供するもの(Chef Server, Ansible Towerなど) Zookeeper/Consul/etcd プログラムによるレジストリエントリの追加，更新，削除をサポートしていること こういうのほしいと前から思っていたけどどうやって実装すればいいか，定義ツールとどう連携させればいいかイメージついてない  軽い構成レジストリ  S3やVCS上のファイル  HTTP等で配布．こうすることで可用性，スケーリングしやすい．管理が単純 頻繁に更新されて複雑になる部分は分割やシャーディングで対応する   こうした場合，例えばAnsibleへはどうやって渡せばいいんだろうか  ansible-playbook実行前にyaml組んでvar_fileなどに渡す  ダイナミックインベントリみたいなことはできなさそう．一回ファイルに吐き出す必要がある？   json組み立ててansible-playbookの--extra-varsオプションに渡す Ansible TowerのAPIでも渡せるかも   Consumer Driven Contract Testing  Itamaeのnode.validate!はまさにこれだと思う1 こんな記事出てきた Consumer-Driven Contracts: A Service Evolution Pattern Pactのようなツールで容易に書けそう    CMDB  CMDBとInfrastructure as Codeは構成管理に対するアプローチが正反対．両者を同一視してはならない  ただしすべてを自動化するならInfrastructure as CodeはCMDBを兼ねることができる．またはInfrastructure as CodeがCMDBも管理することができる ハードウェアも含めてすべてを自動化はけっこうハードル高そう    その他  インフラを完全に管理，自動化するために，やり方を変えるだけでなく自動化しやすいようにタスクそのものを見直すメンタルを忘れてはいけない  以上</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (2章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap2/</link>
      <pubDate>Mon, 12 Jun 2017 20:48:25 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap2/</guid>
      <description>オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．太字は自分の感想， 斜字体は本からの引用 ，そのほかは本の要約など．
ダイナミックインフラストラクチャの要件  NISTのクラウドの要件より広い．  「オンプレミスで，ただひとつのシステムを稼動させるためのインフラ」も考慮に入れているからだろう    プラットフォームから提供されるリソース  計算リソース ストレージリソース  ブロックストレージ オブジェクトストレージ ネットワーク化されたファイルシステム  NFS，SMB  これらのテクノロジは，サーバーが頻繁に追加，削除されるような環境にはうまく適合しない   GlusterFS，HDFS，Ceph  上記の課題に対応できるよう設計されているが，自分の環境でそれがうまくいっていることをきちんとテストすることが重要 代わりにアプリケーションレベルやブロックレベルのレプリケーションで事足りる場合もある       ネットワークリソース  特定のデバイスが高価過ぎて，チームがテストインスタンスを確保できない場合がある．そのような状況に置かれたチームは，優先順位を考えてもっと安いデバイスを使うようにすべきだ  確かに，結局のところ同じハードを同じだけ用意しないとテストできないのだが，そのために使うデバイスを安いものにしろ，という言説は初めて見た．だいたいは仮想化してお茶を濁すのに．      独自クラウドを構築するためのトータルコスト  既存のインフラ，データセンター，知識にかけたコストも自前のホスティングを続ける理由としてよく挙げられる．(中略) しかし，これはサンクコストの呪縛というものだ．  クラウドのポータビリティ  クラウドインフラへの移行を計画するときによく浮上する要件のひとつに，ひとつのクラウドベンダーによる囲い込みを避けるというものがある．(中略) しかし，この要件に時間と金を注ぎ込みすぎないよう注意しなければならない  確かに，クラウドの一部の機能をサードパーティ製のツールで置き換えたところで依然として移行のコストは大きいし，だいたいの場合は運用のコストが上がる あるクラウドでのやり方が(将来にわたって)それが別のクラウドでそのまま利用できるとは限らない  例えば，TerraformでEC2とGCEにインスタンスを作るだけで全く文法が違う(Terraformの批判をしているわけではない) クラウドやツールが将来仕様変更をするかもしれないし   サードパーティ製のものを使うことでよりよりワークフローを得られる可能性がある場合は検討すべき(CodeCommitとCodeBuildの代わりにGithubとTravisCIとか)   自動テストプロセスを継続して維持・使用することで，自信をもって移行を実施できるようにしておくのが現実的な方策  クラウドと仮想マシンに対するマシンレベルの共感  そのプラットフォームで最大のパフォーマンスを引き出すための話？ 必要性は分かるが，ポータビリティとは真逆の話に見える  あまりにそのクラウドに最適化してしまうとポータビリティを落とす要因になりそう   オンプレの場合でも，構成に応じて最適化しつつそれらを管理するのは大変そう．妥協して汎用的なサーバーを横に並べる形になりそう  参考文献  Kief Morris, Infrastructure as Code クラウドにおけるサーバ管理の原則とプラクティス, 長尾高弘訳, オライリー・ジャパン, 2017    Infrastructure as Code ―クラウドにおけるサーバ管理の原則とプラクティス  amazon.</description>
    </item>
    
    <item>
      <title>Infrastructure as Code 感想 (1章)</title>
      <link>https://blog.uyorum.net/post/infrastructure-as-code-chap1/</link>
      <pubDate>Sun, 11 Jun 2017 19:05:42 +0900</pubDate>
      
      <guid>https://blog.uyorum.net/post/infrastructure-as-code-chap1/</guid>
      <description>オライリーの「Infrastructure as Code」を読んで思ったことや自分的メモをまとめておく．太字は自分の感想， 斜字体は本からの引用 ，そのほかは本の要約など．
Infrastructure as Codeの目標  ざっくり言うと「(頻繁な)変化に柔軟に対応できるようになること」ということかなと思った そのために  システム変更が日常茶飯事の出来事になること 失敗を完全に防ぐという前提は捨てる．失敗しても素早く修正できるようになることを目指す 反復的なタスクは自動化すること 継続的な改善をすること   会議やドキュメントでソリューションを論ずることなく，実装，テスト，計測を通じてソリューションの効果が証明できるようになること  Infrastructure as Codeでこれを目指す，というのがピンと来なかった    課題  サーバースプロール，構成ドリフト，スノーフレークサーバーは自分の環境ではほとんどないかな  いちおうChefを使っている ロールごとにVMを作成して役割が混ざらないようにしている   オートメーション恐怖症…これはある  サーバーに統一性がない→オートメーションにより何かが壊れないかが心配→オートメーションツールの外で変更を加える→… の悪循環 この循環から抜けるには「自動化された変更」のリリースプロセスの確立とテストの充実が必要になると最近は考えている  「この変更」を「この方法」で適用するのは安全である，と自動的に言えるようになればいい     ペットから家畜へ  数年前からよく聞くようになった サーバ名にテーマを設け，自分がプロビジョニングした新しいサーバーの名前をじっくり考えていた時代が懐しい．しかし，担当するすべてのサーバーを手作業で調整し，サーバーのご期限をうかがわなけれあならなかった時代は懐しくない  おもしろい      統一的なシステム  一部のサーバーでより大きなディスクが必要になった場合  すべてのサーバーを同じように拡張する xl-file-serverのような新しいロールを追加する   自分だと深く考えずに(ロールを分けずに)そのサーバーだけ拡張してしまいそうだと思った(ディスクサイズだけ変数化しておけばいいじゃん)  その些細な差異が積み重なって管理システムと人に負荷を与えることになる．気をつけたい    反復的なシステム  「パーティションの分割」のような些細なタスクであろうとも，手動でやってしまうと差異が生じる可能性がある．  サイズ，ファイルシステム，そのパラメータ etc.</description>
    </item>
    
  </channel>
</rss>
